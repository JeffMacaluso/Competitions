{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notMNIST - CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:48:14.298042Z",
     "start_time": "2017-11-08T17:47:50.156416Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7945318862555976339\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1526107340\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 1127238059845905014\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 680, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import advanced_activations\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Checking tensorflow processing devices\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:48:16.376253Z",
     "start_time": "2017-11-08T17:48:14.313669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = './notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f,encoding='iso-8859-1')\n",
    "    X_train = save['train_dataset']\n",
    "    y_train = save['train_labels']\n",
    "    X_validation = save['valid_dataset']\n",
    "    y_validation = save['valid_labels']\n",
    "    X_test = save['test_dataset']\n",
    "    y_test = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', X_train.shape, y_train.shape)\n",
    "    print('Validation set', X_validation.shape, y_validation.shape)\n",
    "    print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:48:16.798146Z",
     "start_time": "2017-11-08T17:48:16.407505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (200000, 28, 28, 1)\n",
      "y_train shape: (200000,)\n",
      "X_validation shape: (10000, 28, 28, 1)\n",
      "y_validation shape: (10000,)\n",
      "X_test shape: (10000, 28, 28, 1)\n",
      "y_test shape: (10000,)\n",
      "200000 Train samples\n",
      "10000 Validation samples\n",
      "10000 Test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_validation = X_validation.reshape(X_validation.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_validation = X_validation.reshape(X_validation.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_validation = X_validation.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_validation /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_validation shape:', X_validation.shape)\n",
    "print('y_validation shape:', y_validation.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(X_train.shape[0], 'Train samples')\n",
    "print(X_validation.shape[0], 'Validation samples')\n",
    "print(X_test.shape[0], 'Test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T03:02:35.511428Z",
     "start_time": "2017-11-02T02:57:54.719169Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Augment training data\n",
    "def augment_training_data(images, labels):\n",
    "    \"\"\"\n",
    "    Generates\n",
    "\n",
    "    Creates an additional 300,000 \n",
    "    \n",
    "    Takes ~1.25 minutes with an i7/16gb machine\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty lists to fill\n",
    "    expanded_images = []\n",
    "    expanded_labels = []\n",
    "\n",
    "    # Looping through\n",
    "    j = 0   # counter\n",
    "    for x, y in zip(images, labels):\n",
    "        j = j + 1\n",
    "        if j % 10000 == 0:\n",
    "            print('Expanding data: %03d / %03d' % (j, np.size(images, 0)))\n",
    "\n",
    "        # register original data\n",
    "        expanded_images.append(x)\n",
    "        expanded_labels.append(y)\n",
    "\n",
    "        # get a value for the background\n",
    "        # zero is the expected value, but median() is used to estimate background's value\n",
    "        bg_value = np.median(x)  # this is regarded as background's value\n",
    "        image = np.reshape(x, (-1, 28))\n",
    "\n",
    "        for i in range(4):\n",
    "            # rotate the image with random degree\n",
    "            angle = np.random.randint(-15, 15, 1)\n",
    "            new_img = ndimage.rotate(\n",
    "                image, angle, reshape=False, cval=bg_value)\n",
    "\n",
    "            # shift the image with random distance\n",
    "            shift = np.random.randint(-2, 2, 2)\n",
    "            new_img_ = ndimage.shift(new_img, shift, cval=bg_value)\n",
    "\n",
    "            # register new training data\n",
    "            expanded_images.append(np.reshape(new_img_, (28, 28, 1)))\n",
    "            expanded_labels.append(y)\n",
    "\n",
    "    # images and labels are concatenated for random-shuffle at each epoch\n",
    "    # notice that pair of image and label should not be broken\n",
    "#     expanded_train_total_data = np.concatenate((expanded_images, expanded_labels), axis=1)\n",
    "#     print(np.array(expanded_images).shape)\n",
    "#     print(np.array(expanded_labels).shape)\n",
    "#     np.random.shuffle(expanded_train_total_data)\n",
    "\n",
    "#     return expanded_train_total_data\n",
    "    return expanded_images, expanded_labels\n",
    "\n",
    "\n",
    "print('Starting')\n",
    "augmented = augment_training_data(X_train, y_train)\n",
    "print('Completed')\n",
    "\n",
    "# Appending to the end of the current X/y train\n",
    "X_train_aug = np.append(X_train, augmented[0], axis=0)\n",
    "y_train_aug = np.append(y_train, augmented[1])\n",
    "\n",
    "# Saving as a numpy array\n",
    "print('\\nSaving NP arrays')\n",
    "np.save('X_train_augmented.npy', X_train)\n",
    "np.save('y_train_augmented.npy', y_train)\n",
    "print('Completed')\n",
    "\n",
    "print('\\nX_train shape:', X_train_aug.shape)\n",
    "print('y_train shape:', y_train_aug.shape)\n",
    "print(X_train_aug.shape[0], 'Train samples')\n",
    "print(X_validation.shape[0], 'Validation samples')\n",
    "print(X_test.shape[0], 'Test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading In Augmented Data\n",
    "\n",
    "If not running the data augmentation code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:48:25.376626Z",
     "start_time": "2017-11-08T17:48:16.829397Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_aug shape: (1200000, 28, 28, 1)\n",
      "y_train_aug shape: (1200000,)\n",
      "1200000 Train samples\n",
      "10000 Validation samples\n",
      "10000 Test samples\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if not performing data augmentation here\n",
    "X_train_aug = np.load('X_train_augmented.npy')\n",
    "y_train_aug = np.load('y_train_augmented.npy')\n",
    "\n",
    "print('X_train_aug shape:', X_train_aug.shape)\n",
    "print('y_train_aug shape:', y_train_aug.shape)\n",
    "print(X_train_aug.shape[0], 'Train samples')\n",
    "print(X_validation.shape[0], 'Validation samples')\n",
    "print(X_test.shape[0], 'Test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training on Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T17:48:25.501631Z",
     "start_time": "2017-11-08T17:48:25.439129Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding for keras input\n",
    "num_classes = 10\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_train_aug = keras.utils.np_utils.to_categorical(y_train_aug, num_classes)\n",
    "y_validation = keras.utils.np_utils.to_categorical(y_validation, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-11-05T23:07:45.612Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "1200000/1200000 [==============================] - 206s - loss: 0.4686 - acc: 0.8629 - val_loss: 0.4485 - val_acc: 0.8646\n",
      "Epoch 2/20\n",
      "1200000/1200000 [==============================] - 206s - loss: 0.3713 - acc: 0.8903 - val_loss: 0.4509 - val_acc: 0.8586\n",
      "Epoch 3/20\n",
      "1200000/1200000 [==============================] - 206s - loss: 0.3439 - acc: 0.8978 - val_loss: 0.3027 - val_acc: 0.9093\n",
      "Epoch 4/20\n",
      "1200000/1200000 [==============================] - 206s - loss: 0.3276 - acc: 0.9024 - val_loss: 0.3370 - val_acc: 0.9061\n",
      "Epoch 5/20\n",
      "1200000/1200000 [==============================] - 205s - loss: 0.3153 - acc: 0.9055 - val_loss: 0.3041 - val_acc: 0.9153\n",
      "Epoch 6/20\n",
      "1200000/1200000 [==============================] - 204s - loss: 0.3063 - acc: 0.9081 - val_loss: 0.2942 - val_acc: 0.9151\n",
      "Epoch 7/20\n",
      "1200000/1200000 [==============================] - 206s - loss: 0.2990 - acc: 0.9101 - val_loss: 0.2831 - val_acc: 0.9168\n",
      "Epoch 8/20\n",
      "1200000/1200000 [==============================] - 213s - loss: 0.2930 - acc: 0.9118 - val_loss: 0.2882 - val_acc: 0.9182\n",
      "Epoch 9/20\n",
      "1200000/1200000 [==============================] - 278s - loss: 0.2871 - acc: 0.9131 - val_loss: 0.2992 - val_acc: 0.9141\n",
      "Epoch 10/20\n",
      "1200000/1200000 [==============================] - 289s - loss: 0.2817 - acc: 0.9148 - val_loss: 0.2889 - val_acc: 0.9130\n",
      "Epoch 11/20\n",
      "1200000/1200000 [==============================] - 240s - loss: 0.2772 - acc: 0.9159 - val_loss: 0.2711 - val_acc: 0.9228\n",
      "Epoch 12/20\n",
      " 758912/1200000 [=================>............] - ETA: 88s - loss: 0.2736 - acc: 0.9169"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "## Constructing the model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                 input_shape=input_shape))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Conv2D(64, (5, 5)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Third hidden layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_aug, y_train_aug,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_validation, y_validation))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training on non-Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-05T23:00:59.256307Z",
     "start_time": "2017-11-05T22:49:43.855893Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "200000/200000 [==============================] - 33s - loss: 0.5558 - acc: 0.8405 - val_loss: 0.4321 - val_acc: 0.8739\n",
      "Epoch 2/20\n",
      "200000/200000 [==============================] - 32s - loss: 0.4045 - acc: 0.8834 - val_loss: 0.4068 - val_acc: 0.8799\n",
      "Epoch 3/20\n",
      "200000/200000 [==============================] - 33s - loss: 0.3720 - acc: 0.8922 - val_loss: 0.3950 - val_acc: 0.8823\n",
      "Epoch 4/20\n",
      "200000/200000 [==============================] - 33s - loss: 0.3509 - acc: 0.8973 - val_loss: 0.6630 - val_acc: 0.8100\n",
      "Epoch 5/20\n",
      "200000/200000 [==============================] - 33s - loss: 0.3348 - acc: 0.9023 - val_loss: 0.3453 - val_acc: 0.8974\n",
      "Epoch 6/20\n",
      "200000/200000 [==============================] - 33s - loss: 0.3231 - acc: 0.9055 - val_loss: 0.4066 - val_acc: 0.8802\n",
      "Epoch 7/20\n",
      "200000/200000 [==============================] - 34s - loss: 0.3138 - acc: 0.9082 - val_loss: 0.3538 - val_acc: 0.8932\n",
      "Epoch 8/20\n",
      "200000/200000 [==============================] - 34s - loss: 0.3043 - acc: 0.9109 - val_loss: 0.3274 - val_acc: 0.9013\n",
      "Epoch 9/20\n",
      "200000/200000 [==============================] - 32s - loss: 0.2978 - acc: 0.9124 - val_loss: 0.3552 - val_acc: 0.8944\n",
      "Epoch 10/20\n",
      "200000/200000 [==============================] - 32s - loss: 0.2896 - acc: 0.9147 - val_loss: 0.4325 - val_acc: 0.8737\n",
      "Epoch 11/20\n",
      "200000/200000 [==============================] - 33s - loss: 0.2847 - acc: 0.9158 - val_loss: 0.3128 - val_acc: 0.9102\n",
      "Epoch 12/20\n",
      "200000/200000 [==============================] - 34s - loss: 0.2791 - acc: 0.9177 - val_loss: 0.3290 - val_acc: 0.9047\n",
      "Epoch 13/20\n",
      "200000/200000 [==============================] - 34s - loss: 0.2742 - acc: 0.9187 - val_loss: 0.3499 - val_acc: 0.8989\n",
      "Epoch 14/20\n",
      "200000/200000 [==============================] - 34s - loss: 0.2689 - acc: 0.9204 - val_loss: 0.4158 - val_acc: 0.8883\n",
      "Epoch 15/20\n",
      "200000/200000 [==============================] - 33s - loss: 0.2634 - acc: 0.9217 - val_loss: 0.3419 - val_acc: 0.9001\n",
      "Epoch 16/20\n",
      "200000/200000 [==============================] - 33s - loss: 0.2581 - acc: 0.9237 - val_loss: 0.2990 - val_acc: 0.9134\n",
      "Epoch 17/20\n",
      "200000/200000 [==============================] - 33s - loss: 0.2541 - acc: 0.9239 - val_loss: 0.3288 - val_acc: 0.9043\n",
      "Epoch 18/20\n",
      "200000/200000 [==============================] - 34s - loss: 0.2500 - acc: 0.9254 - val_loss: 0.3305 - val_acc: 0.9041\n",
      "Epoch 19/20\n",
      "200000/200000 [==============================] - 34s - loss: 0.2460 - acc: 0.9261 - val_loss: 0.3285 - val_acc: 0.9043\n",
      "Epoch 20/20\n",
      "200000/200000 [==============================] - 34s - loss: 0.2420 - acc: 0.9269 - val_loss: 0.3410 - val_acc: 0.9053\n",
      "\n",
      "Test loss: 0.14369408296\n",
      "Test accuracy: 0.9595\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "## Constructing the model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                 input_shape=input_shape))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "# model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Conv2D(64, (5, 5)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Third hidden layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_validation, y_validation))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "# 0.9626 with batch norm on second layer only\n",
    "# Note: Batch norm on the first layer tends to have poor performance for some reason"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training - Different Architectures w/ out augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-08T20:47:46.854906Z",
     "start_time": "2017-11-08T19:40:03.424885Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "200000/200000 [==============================] - 204s - loss: 0.7616 - acc: 0.7603 - val_loss: 0.5280 - val_acc: 0.8443\n",
      "Epoch 2/20\n",
      "200000/200000 [==============================] - 202s - loss: 0.5012 - acc: 0.8477 - val_loss: 0.4010 - val_acc: 0.8749\n",
      "Epoch 3/20\n",
      "200000/200000 [==============================] - 202s - loss: 0.4381 - acc: 0.8664 - val_loss: 0.3666 - val_acc: 0.8876\n",
      "Epoch 4/20\n",
      "200000/200000 [==============================] - 201s - loss: 0.4018 - acc: 0.8770 - val_loss: 0.3533 - val_acc: 0.8929\n",
      "Epoch 5/20\n",
      "200000/200000 [==============================] - 201s - loss: 0.3774 - acc: 0.8844 - val_loss: 0.3173 - val_acc: 0.8971\n",
      "Epoch 6/20\n",
      "200000/200000 [==============================] - 202s - loss: 0.3605 - acc: 0.8894 - val_loss: 0.3047 - val_acc: 0.9051\n",
      "Epoch 7/20\n",
      "200000/200000 [==============================] - 202s - loss: 0.3446 - acc: 0.8935 - val_loss: 0.3002 - val_acc: 0.9072\n",
      "Epoch 8/20\n",
      "200000/200000 [==============================] - 203s - loss: 0.3338 - acc: 0.8973 - val_loss: 0.2801 - val_acc: 0.9134\n",
      "Epoch 9/20\n",
      "200000/200000 [==============================] - 203s - loss: 0.3230 - acc: 0.8998 - val_loss: 0.2777 - val_acc: 0.9121\n",
      "Epoch 10/20\n",
      "200000/200000 [==============================] - 203s - loss: 0.3142 - acc: 0.9023 - val_loss: 0.2719 - val_acc: 0.9147\n",
      "Epoch 11/20\n",
      "200000/200000 [==============================] - 203s - loss: 0.3063 - acc: 0.9054 - val_loss: 0.2704 - val_acc: 0.9143\n",
      "Epoch 12/20\n",
      "200000/200000 [==============================] - 202s - loss: 0.3012 - acc: 0.9066 - val_loss: 0.2686 - val_acc: 0.9157\n",
      "Epoch 13/20\n",
      "200000/200000 [==============================] - 203s - loss: 0.2940 - acc: 0.9089 - val_loss: 0.2592 - val_acc: 0.9193\n",
      "Epoch 14/20\n",
      "200000/200000 [==============================] - 202s - loss: 0.2883 - acc: 0.9103 - val_loss: 0.2617 - val_acc: 0.9191\n",
      "Epoch 15/20\n",
      "200000/200000 [==============================] - 201s - loss: 0.2832 - acc: 0.9122 - val_loss: 0.2607 - val_acc: 0.9163\n",
      "Epoch 16/20\n",
      "200000/200000 [==============================] - 201s - loss: 0.2768 - acc: 0.9134 - val_loss: 0.2542 - val_acc: 0.9213\n",
      "Epoch 17/20\n",
      "200000/200000 [==============================] - 203s - loss: 0.2745 - acc: 0.9142 - val_loss: 0.2484 - val_acc: 0.9216\n",
      "Epoch 18/20\n",
      "200000/200000 [==============================] - 203s - loss: 0.2697 - acc: 0.9158 - val_loss: 0.2500 - val_acc: 0.9221\n",
      "Epoch 19/20\n",
      "200000/200000 [==============================] - 203s - loss: 0.2643 - acc: 0.9177 - val_loss: 0.2458 - val_acc: 0.9245\n",
      "Epoch 20/20\n",
      "200000/200000 [==============================] - 202s - loss: 0.2603 - acc: 0.9187 - val_loss: 0.2511 - val_acc: 0.9223\n",
      "\n",
      "Test loss: 0.101749044681\n",
      "Test accuracy: 0.9701\n"
     ]
    }
   ],
   "source": [
    "# Simple convnet achieved 98.19% on test, code here: https://github.com/alex-petrenko/udacity-deep-learning/blob/14714ee4151b798cde0a31a94ac65e08b87d0f65/assignment_04_convolutions.py#L39\n",
    "# (5,5)->(5,5)->pool->(3,3)->(3,3)->pool->fc1024->fc1024->logits\n",
    "# INFO Starting new epoch #121!\n",
    "# INFO Minibatch loss: 0.150696, reg loss: 0.041653, accuracy: 96.88%\n",
    "# INFO Train loss: 0.068505, train accuracy: 99.28%\n",
    "# INFO Test loss: 0.118685, TEST ACCURACY: 98.05% BEST ACCURACY 98.19% <<<<<<<\n",
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Layer 1\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), input_shape=input_shape))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "# model.add(BatchNormalization(axis=1))\n",
    "\n",
    "# Layer 2\n",
    "model.add(Conv2D(32, kernel_size=(5, 5)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "# model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 3\n",
    "model.add(Conv2D(64, kernel_size=(3, 3)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "model.add(BatchNormalization(axis=1))\n",
    "\n",
    "# Layer 4\n",
    "model.add(Conv2D(64, kernel_size=(3, 3)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.03))\n",
    "# model.add(BatchNormalization(axis=1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Layer 5\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# Layer 6\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_validation, y_validation))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('\\nTest loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perf without augmented dataset and on 20 epochs:\n",
    "\n",
    "- all batch normalization: 0.8766\n",
    "- no batch normalization: 0.9647(?)\n",
    "- batch normalization on the second layer only: 0.964\n",
    "- batch normalization on the third layer (w/ axis=1): 0.9701"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test accuracy: 98.0%\n",
    "# Implementation:\n",
    "# 2 CNNs with max pooling followed by a 1 layer fully-connected NN:\n",
    "# Patch size = 5x5\n",
    "# Stride for CNN = 1\n",
    "# Size of pooling size = 2x2\n",
    "# Stride for Pooling = 2\n",
    "# Depth = 50, 100\n",
    "# Hidden layer Nodes in FCNN = 512\n",
    "# Dropout_keep_probe = init: 95%, decaying to 70% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T16:05:20.630142Z",
     "start_time": "2017-11-04T16:05:20.614517Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# est accuracy: 97.2%\n",
    "# Implementation:\n",
    "# 2 CNNs with max pooling followed by a 1 layer fully-connected NN:\n",
    "# Patch size = 7x7\n",
    "# Stride for CNN = 1\n",
    "# Size of pooling size = 2x2\n",
    "# Stride for Pooling = 2\n",
    "# Depth = 50, 100\n",
    "# Final layer nodes = 512\n",
    "# Dropout_keep_probe = 0.7 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
