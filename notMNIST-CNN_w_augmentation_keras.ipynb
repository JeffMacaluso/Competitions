{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## notMNIST - CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T15:36:36.639253Z",
     "start_time": "2017-11-04T15:36:13.823220Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14807064050257912011\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1526107340\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 16138876657672656534\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 680, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import advanced_activations\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Checking tensorflow processing devices\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T15:40:49.067197Z",
     "start_time": "2017-11-04T15:40:48.264961Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'C:/Users/Macal/Documents/Projects/Competitions/MNIST/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f,encoding='iso-8859-1')\n",
    "    X_train = save['train_dataset']\n",
    "    y_train = save['train_labels']\n",
    "    X_validation = save['valid_dataset']\n",
    "    y_validation = save['valid_labels']\n",
    "    X_test = save['test_dataset']\n",
    "    y_test = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', X_train.shape, y_train.shape)\n",
    "    print('Validation set', X_validation.shape, y_validation.shape)\n",
    "    print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T15:40:49.533007Z",
     "start_time": "2017-11-04T15:40:49.115283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (200000, 28, 28, 1)\n",
      "y_train shape: (200000,)\n",
      "X_validation shape: (10000, 28, 28, 1)\n",
      "y_validation shape: (10000,)\n",
      "X_test shape: (10000, 28, 28, 1)\n",
      "y_test shape: (10000,)\n",
      "200000 Train samples\n",
      "10000 Validation samples\n",
      "10000 Test samples\n"
     ]
    }
   ],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_validation = X_validation.reshape(X_validation.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_validation = X_validation.reshape(X_validation.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_validation = X_validation.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_validation /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('X_validation shape:', X_validation.shape)\n",
    "print('y_validation shape:', y_validation.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_test shape:', y_test.shape)\n",
    "print(X_train.shape[0], 'Train samples')\n",
    "print(X_validation.shape[0], 'Validation samples')\n",
    "print(X_test.shape[0], 'Test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-02T03:02:35.511428Z",
     "start_time": "2017-11-02T02:57:54.719169Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Augment training data\n",
    "def augment_training_data(images, labels):\n",
    "    \"\"\"\n",
    "    Generates\n",
    "\n",
    "    Creates an additional 300,000 \n",
    "    \n",
    "    Takes ~1.25 minutes with an i7/16gb machine\n",
    "    \"\"\"\n",
    "\n",
    "    # Empty lists to fill\n",
    "    expanded_images = []\n",
    "    expanded_labels = []\n",
    "\n",
    "    # Looping through\n",
    "    j = 0   # counter\n",
    "    for x, y in zip(images, labels):\n",
    "        j = j + 1\n",
    "        if j % 10000 == 0:\n",
    "            print('Expanding data: %03d / %03d' % (j, np.size(images, 0)))\n",
    "\n",
    "        # register original data\n",
    "        expanded_images.append(x)\n",
    "        expanded_labels.append(y)\n",
    "\n",
    "        # get a value for the background\n",
    "        # zero is the expected value, but median() is used to estimate background's value\n",
    "        bg_value = np.median(x)  # this is regarded as background's value\n",
    "        image = np.reshape(x, (-1, 28))\n",
    "\n",
    "        for i in range(4):\n",
    "            # rotate the image with random degree\n",
    "            angle = np.random.randint(-15, 15, 1)\n",
    "            new_img = ndimage.rotate(\n",
    "                image, angle, reshape=False, cval=bg_value)\n",
    "\n",
    "            # shift the image with random distance\n",
    "            shift = np.random.randint(-2, 2, 2)\n",
    "            new_img_ = ndimage.shift(new_img, shift, cval=bg_value)\n",
    "\n",
    "            # register new training data\n",
    "            expanded_images.append(np.reshape(new_img_, (28, 28, 1)))\n",
    "            expanded_labels.append(y)\n",
    "\n",
    "    # images and labels are concatenated for random-shuffle at each epoch\n",
    "    # notice that pair of image and label should not be broken\n",
    "#     expanded_train_total_data = np.concatenate((expanded_images, expanded_labels), axis=1)\n",
    "#     print(np.array(expanded_images).shape)\n",
    "#     print(np.array(expanded_labels).shape)\n",
    "#     np.random.shuffle(expanded_train_total_data)\n",
    "\n",
    "#     return expanded_train_total_data\n",
    "    return expanded_images, expanded_labels\n",
    "\n",
    "\n",
    "print('Starting')\n",
    "augmented = augment_training_data(X_train, y_train)\n",
    "print('Completed')\n",
    "\n",
    "# Appending to the end of the current X/y train\n",
    "X_train_aug = np.append(X_train, augmented[0], axis=0)\n",
    "y_train_aug = np.append(y_train, augmented[1])\n",
    "\n",
    "# Saving as a numpy array\n",
    "print('\\nSaving NP arrays')\n",
    "np.save('X_train_augmented.npy', X_train)\n",
    "np.save('y_train_augmented.npy', y_train)\n",
    "print('Completed')\n",
    "\n",
    "print('\\nX_train shape:', X_train_aug.shape)\n",
    "print('y_train shape:', y_train_aug.shape)\n",
    "print(X_train_aug.shape[0], 'Train samples')\n",
    "print(X_validation.shape[0], 'Validation samples')\n",
    "print(X_test.shape[0], 'Test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading In Augmented Data\n",
    "\n",
    "If not running the data augmentation code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T15:40:54.069078Z",
     "start_time": "2017-11-04T15:40:52.450063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_aug shape: (1200000, 28, 28, 1)\n",
      "y_train_aug shape: (1200000,)\n",
      "1200000 Train samples\n",
      "10000 Validation samples\n",
      "10000 Test samples\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if not performing data augmentation here\n",
    "X_train_aug = np.load('X_train_augmented.npy')\n",
    "y_train_aug = np.load('y_train_augmented.npy')\n",
    "\n",
    "print('X_train_aug shape:', X_train_aug.shape)\n",
    "print('y_train_aug shape:', y_train_aug.shape)\n",
    "print(X_train_aug.shape[0], 'Train samples')\n",
    "print(X_validation.shape[0], 'Validation samples')\n",
    "print(X_test.shape[0], 'Test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training on Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T15:41:57.113276Z",
     "start_time": "2017-11-04T15:41:57.043756Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot encoding for keras input\n",
    "num_classes = 10\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "y_train_aug = keras.utils.np_utils.to_categorical(y_train_aug, num_classes)\n",
    "y_validation = keras.utils.np_utils.to_categorical(y_validation, num_classes)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T00:06:39.919459Z",
     "start_time": "2017-11-03T23:03:39.389275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1200000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "1200000/1200000 [==============================] - 164s - loss: 0.6342 - acc: 0.8089 - val_loss: 0.4062 - val_acc: 0.8776\n",
      "Epoch 2/20\n",
      "1200000/1200000 [==============================] - 161s - loss: 0.4529 - acc: 0.8667 - val_loss: 0.3555 - val_acc: 0.8960\n",
      "Epoch 3/20\n",
      "1200000/1200000 [==============================] - 162s - loss: 0.4103 - acc: 0.8788 - val_loss: 0.3301 - val_acc: 0.8995\n",
      "Epoch 4/20\n",
      "1200000/1200000 [==============================] - 166s - loss: 0.3838 - acc: 0.8862 - val_loss: 0.3137 - val_acc: 0.9052\n",
      "Epoch 5/20\n",
      "1200000/1200000 [==============================] - 165s - loss: 0.3647 - acc: 0.8917 - val_loss: 0.3057 - val_acc: 0.9097\n",
      "Epoch 6/20\n",
      "1200000/1200000 [==============================] - 176s - loss: 0.3496 - acc: 0.8962 - val_loss: 0.2941 - val_acc: 0.9120\n",
      "Epoch 7/20\n",
      "1200000/1200000 [==============================] - 204s - loss: 0.3377 - acc: 0.8992 - val_loss: 0.2860 - val_acc: 0.9155\n",
      "Epoch 8/20\n",
      "1200000/1200000 [==============================] - 208s - loss: 0.3280 - acc: 0.9021 - val_loss: 0.2813 - val_acc: 0.9175\n",
      "Epoch 9/20\n",
      "1200000/1200000 [==============================] - 190s - loss: 0.3196 - acc: 0.9044 - val_loss: 0.2768 - val_acc: 0.9183\n",
      "Epoch 10/20\n",
      "1200000/1200000 [==============================] - 180s - loss: 0.3123 - acc: 0.9064 - val_loss: 0.2735 - val_acc: 0.9181\n",
      "Epoch 11/20\n",
      "1200000/1200000 [==============================] - 210s - loss: 0.3061 - acc: 0.9081 - val_loss: 0.2677 - val_acc: 0.9198\n",
      "Epoch 12/20\n",
      "1200000/1200000 [==============================] - 193s - loss: 0.3010 - acc: 0.9095 - val_loss: 0.2679 - val_acc: 0.9189\n",
      "Epoch 13/20\n",
      "1200000/1200000 [==============================] - 203s - loss: 0.2959 - acc: 0.9108 - val_loss: 0.2614 - val_acc: 0.9213\n",
      "Epoch 14/20\n",
      "1200000/1200000 [==============================] - 211s - loss: 0.2917 - acc: 0.9121 - val_loss: 0.2621 - val_acc: 0.9221.2917 - acc:  - ETA: 2s - loss: 0.2917 - a - ETA: 2s - loss: 0.2917 - acc:  - ETA: 1s \n",
      "Epoch 15/20\n",
      "1200000/1200000 [==============================] - 200s - loss: 0.2875 - acc: 0.9131 - val_loss: 0.2597 - val_acc: 0.9220\n",
      "Epoch 16/20\n",
      "1200000/1200000 [==============================] - 200s - loss: 0.2843 - acc: 0.9141 - val_loss: 0.2608 - val_acc: 0.9224\n",
      "Epoch 17/20\n",
      "1200000/1200000 [==============================] - 189s - loss: 0.2806 - acc: 0.9151 - val_loss: 0.2577 - val_acc: 0.9233\n",
      "Epoch 18/20\n",
      "1200000/1200000 [==============================] - 190s - loss: 0.2779 - acc: 0.9159 - val_loss: 0.2561 - val_acc: 0.9241\n",
      "Epoch 19/20\n",
      "1200000/1200000 [==============================] - 196s - loss: 0.2750 - acc: 0.9166 - val_loss: 0.2525 - val_acc: 0.9254\n",
      "Epoch 20/20\n",
      "1200000/1200000 [==============================] - 201s - loss: 0.2720 - acc: 0.9174 - val_loss: 0.2568 - val_acc: 0.9248\n",
      "Test loss: 0.102421606927\n",
      "Test accuracy: 0.9698\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "\n",
    "## Constructing the model\n",
    "model = Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "model.add(Conv2D(32, kernel_size=(5, 5),\n",
    "                 input_shape=input_shape))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Conv2D(64, (5, 5)))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Third hidden layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(advanced_activations.LeakyReLU(alpha=0.3))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Output Layer\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train_aug, y_train_aug,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_validation, y_validation))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training on non-Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-11-04T15:52:37.973726Z",
     "start_time": "2017-11-04T15:43:15.103971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "200000/200000 [==============================] - 30s - loss: 1.0068 - acc: 0.6808 - val_loss: 0.5768 - val_acc: 0.8252\n",
      "Epoch 2/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.5396 - acc: 0.8430 - val_loss: 0.4852 - val_acc: 0.8557\n",
      "Epoch 3/20\n",
      "200000/200000 [==============================] - 28s - loss: 0.4784 - acc: 0.8609 - val_loss: 0.4461 - val_acc: 0.8699\n",
      "Epoch 4/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.4457 - acc: 0.8709 - val_loss: 0.4164 - val_acc: 0.8745\n",
      "Epoch 5/20\n",
      "200000/200000 [==============================] - 28s - loss: 0.4236 - acc: 0.8770 - val_loss: 0.3951 - val_acc: 0.8835\n",
      "Epoch 6/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.4059 - acc: 0.8818 - val_loss: 0.3808 - val_acc: 0.8862\n",
      "Epoch 7/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.3922 - acc: 0.8859 - val_loss: 0.3729 - val_acc: 0.8877\n",
      "Epoch 8/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.3806 - acc: 0.8887 - val_loss: 0.3609 - val_acc: 0.8924\n",
      "Epoch 9/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.3706 - acc: 0.8919 - val_loss: 0.3487 - val_acc: 0.8962\n",
      "Epoch 10/20\n",
      "200000/200000 [==============================] - 28s - loss: 0.3620 - acc: 0.8940 - val_loss: 0.3448 - val_acc: 0.8944\n",
      "Epoch 11/20\n",
      "200000/200000 [==============================] - 29s - loss: 0.3541 - acc: 0.8966 - val_loss: 0.3486 - val_acc: 0.8951\n",
      "Epoch 12/20\n",
      "200000/200000 [==============================] - 28s - loss: 0.3471 - acc: 0.8981 - val_loss: 0.3437 - val_acc: 0.8956\n",
      "Epoch 13/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.3411 - acc: 0.9000 - val_loss: 0.3278 - val_acc: 0.9011\n",
      "Epoch 14/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.3361 - acc: 0.9011 - val_loss: 0.3271 - val_acc: 0.9017\n",
      "Epoch 15/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.3308 - acc: 0.9024 - val_loss: 0.3283 - val_acc: 0.9030\n",
      "Epoch 16/20\n",
      "200000/200000 [==============================] - 28s - loss: 0.3262 - acc: 0.9043 - val_loss: 0.3179 - val_acc: 0.9046\n",
      "Epoch 17/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.3212 - acc: 0.9052 - val_loss: 0.3322 - val_acc: 0.9013\n",
      "Epoch 18/20\n",
      "200000/200000 [==============================] - 27s - loss: 0.3177 - acc: 0.9060 - val_loss: 0.3101 - val_acc: 0.9062\n",
      "Epoch 19/20\n",
      "200000/200000 [==============================] - 28s - loss: 0.3131 - acc: 0.9079 - val_loss: 0.3101 - val_acc: 0.9079\n",
      "Epoch 20/20\n",
      "200000/200000 [==============================] - 28s - loss: 0.3095 - acc: 0.9084 - val_loss: 0.3061 - val_acc: 0.9089\n",
      "Test loss: 0.142385928509\n",
      "Test accuracy: 0.9588\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_validation, y_validation))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
